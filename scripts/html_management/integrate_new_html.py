#!/usr/bin/env python3
"""
AWS SAP å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹è‡ªå‹•çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

new_html/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•çš„ã«åˆ†æã—ã€
é©åˆ‡ãªã‚«ãƒ†ã‚´ãƒªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã—ã¦index.htmlã‚’æ›´æ–°ã—ã¾ã™ã€‚
"""

import os
import re
import json
import shutil
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from html.parser import HTMLParser
import argparse


class HTMLTitleParser(HTMLParser):
    """HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ¡ã‚¿æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ãƒ‘ãƒ¼ã‚µãƒ¼"""

    def __init__(self):
        super().__init__()
        self.title = ""
        self.in_title = False
        self.h1_text = ""
        self.in_h1 = False

    def handle_starttag(self, tag, attrs):
        if tag == "title":
            self.in_title = True
        elif tag == "h1":
            self.in_h1 = True

    def handle_endtag(self, tag):
        if tag == "title":
            self.in_title = False
        elif tag == "h1":
            self.in_h1 = False

    def handle_data(self, data):
        if self.in_title:
            self.title += data.strip()
        elif self.in_h1 and not self.h1_text:
            self.h1_text = data.strip()


class HTMLIntegrator:
    """HTMLãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆã®è‡ªå‹•åŒ–ã‚¯ãƒ©ã‚¹"""

    # ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ â†’ ã‚«ãƒ†ã‚´ãƒªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
    CATEGORY_MAPPINGS = {
        "security-governance": {
            "keywords": [
                "iam", "cognito", "scp", "organizations", "config", "control tower",
                "guardrails", "cmk", "kms", "æš—å·", "èªè¨¼", "èªå¯", "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£",
                "ã‚¬ãƒãƒŠãƒ³ã‚¹", "waf", "shield", "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£è¦–", "è„…å¨æ¤œçŸ¥"
            ],
            "section": "Organizations & ã‚¬ãƒãƒŠãƒ³ã‚¹",
            "icon": "ğŸ¢"
        },
        "compute-applications": {
            "keywords": [
                "ec2", "auto scaling", "autoscaling", "lambda", "ecs", "fargate",
                "alb", "elb", "sqs", "sns", "ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹", "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆ",
                "ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³", "ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°", "ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°", "lifecycle",
                "warm pool", "patch manager", "systems manager"
            ],
            "section": "Auto Scaling & ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°",
            "icon": "âš–ï¸"
        },
        "content-delivery-dns": {
            "keywords": [
                "cloudfront", "route53", "dns", "global accelerator", "cdn",
                "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é…ä¿¡", "ã‚­ãƒ£ãƒƒã‚·ãƒ¥", "https", "ssl", "tls", "è¨¼æ˜æ›¸"
            ],
            "section": "CloudFront & ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é…ä¿¡",
            "icon": "âš¡"
        },
        "networking": {
            "keywords": [
                "vpc", "direct connect", "vpn", "transit gateway", "tgw",
                "privatelink", "eni", "eip", "nat", "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "æ¥ç¶š"
            ],
            "section": "VPC & ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åŸºç¤",
            "icon": "ğŸ—ï¸"
        },
        "storage-database": {
            "keywords": [
                "s3", "ebs", "efs", "fsx", "rds", "aurora", "dynamodb", "redshift",
                "elasticache", "ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸", "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°"
            ],
            "section": "S3 & ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸",
            "icon": "ğŸª£"
        },
        "development-deployment": {
            "keywords": [
                "cloudformation", "cdk", "sam", "codepipeline", "codedeploy",
                "eventbridge", "api gateway", "appsync", "é–‹ç™º", "ãƒ‡ãƒ—ãƒ­ã‚¤",
                "iac", "ci/cd"
            ],
            "section": "IaC & CloudFormation",
            "icon": "ğŸ“œ"
        },
        "migration-transfer": {
            "keywords": [
                "dms", "migration hub", "sct", "ç§»è¡Œ", "ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³",
                "dr", "ãƒ‡ã‚£ã‚¶ã‚¹ã‚¿ãƒªã‚«ãƒãƒª", "disaster recovery"
            ],
            "section": "DMS & ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ",
            "icon": "ğŸ”„"
        },
        "organizational-complexity": {
            "keywords": [
                "ram", "resource access manager", "service catalog", "stacksets",
                "cfct", "çµ„ç¹”", "ãƒãƒ«ãƒã‚¢ã‚«ã‚¦ãƒ³ãƒˆ", "å…±æœ‰"
            ],
            "section": "Organizations & ã‚¬ãƒãƒŠãƒ³ã‚¹",
            "icon": "ğŸ¢"
        },
        "continuous-improvement": {
            "keywords": [
                "cloudwatch", "cloudtrail", "systems manager", "ssm", "x-ray",
                "é‹ç”¨", "ç›£è¦–", "æ”¹å–„", "ãƒ‘ãƒƒãƒ", "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰"
            ],
            "section": "ã‚·ã‚¹ãƒ†ãƒ é‹ç”¨ & ãƒ‘ãƒƒãƒç®¡ç†",
            "icon": "ğŸ”§"
        }
    }

    def __init__(self, source_dir: str = "new_html", dry_run: bool = False):
        self.source_dir = Path(source_dir)
        self.dry_run = dry_run
        self.repo_root = Path(__file__).parent
        self.index_html = self.repo_root / "index.html"
        self.moved_files: List[Dict] = []

    def analyze_html_file(self, file_path: Path) -> Dict:
        """HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æã—ã¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            parser = HTMLTitleParser()
            parser.feed(content)

            # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
            title = parser.title or parser.h1_text or file_path.stem
            keywords = self._extract_keywords(title.lower() + " " + content.lower())

            return {
                "filename": file_path.name,
                "title": title,
                "h1": parser.h1_text,
                "keywords": keywords,
                "size": file_path.stat().st_size,
                "lines": len(content.splitlines())
            }
        except Exception as e:
            print(f"âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼ {file_path.name}: {e}")
            return None

    def _extract_keywords(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰AWSé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keywords = []
        all_keywords = set()

        for category_keywords in self.CATEGORY_MAPPINGS.values():
            all_keywords.update(kw.lower() for kw in category_keywords["keywords"])

        for keyword in all_keywords:
            if keyword in text:
                keywords.append(keyword)

        return keywords

    def determine_category(self, metadata: Dict) -> Tuple[str, str, str]:
        """
        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’åˆ¤å®š
        Returns: (category_dir, section_name, icon)
        """
        keyword_scores = {}

        # å„ã‚«ãƒ†ã‚´ãƒªã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        for category, config in self.CATEGORY_MAPPINGS.items():
            score = 0
            for keyword in metadata["keywords"]:
                if keyword.lower() in [kw.lower() for kw in config["keywords"]]:
                    score += 1
            keyword_scores[category] = score

        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ
        if keyword_scores:
            best_category = max(keyword_scores, key=keyword_scores.get)
            if keyword_scores[best_category] > 0:
                config = self.CATEGORY_MAPPINGS[best_category]
                return best_category, config["section"], config["icon"]

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: compute-applications
        return "compute-applications", "ã‚³ãƒ³ãƒ†ãƒŠ & ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ±åˆ", "ğŸ“¦"

    def move_file(self, source: Path, category: str) -> Optional[Path]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é©åˆ‡ãªã‚«ãƒ†ã‚´ãƒªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•"""
        dest_dir = self.repo_root / category
        dest_file = dest_dir / source.name

        if not self.dry_run:
            # Zone.Identifierãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            zone_file = Path(str(source) + ":Zone.Identifier")
            if zone_file.exists():
                zone_file.unlink()
                print(f"   ğŸ—‘ï¸  Zone.Identifierå‰Šé™¤: {zone_file.name}")

            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            dest_dir.mkdir(parents=True, exist_ok=True)

            # ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•
            shutil.move(str(source), str(dest_file))
            print(f"   âœ… ç§»å‹•å®Œäº†: {source.name} â†’ {category}/")
        else:
            print(f"   [DRY RUN] ç§»å‹•: {source.name} â†’ {category}/")

        return dest_file

    def update_index_html(self, files_info: List[Dict]) -> bool:
        """index.htmlã‚’æ›´æ–°ã—ã¦æ–°ã—ã„ãƒªã‚½ãƒ¼ã‚¹ã‚’è¿½åŠ """
        if not self.index_html.exists():
            print("âŒ index.html ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False

        try:
            with open(self.index_html, 'r', encoding='utf-8') as f:
                content = f.read()

            original_content = content

            # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            category_groups = {}
            for info in files_info:
                category = info["category"]
                if category not in category_groups:
                    category_groups[category] = []
                category_groups[category].append(info)

            # å„ã‚«ãƒ†ã‚´ãƒªã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ›´æ–°
            for category, files in category_groups.items():
                section_name = files[0]["section"]

                # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¦‹ã¤ã‘ã¦æ›´æ–°
                pattern = rf'(<h2><span class="section-icon">[^<]+</span>{re.escape(section_name)}<span class="resource-count">)(\d+)(</span></h2>\s*<ul class="resource-list">)'

                def replace_section(match):
                    current_count = int(match.group(2))
                    new_count = current_count + len(files)

                    # æ–°ã—ã„ãƒªã‚½ãƒ¼ã‚¹é …ç›®ã‚’è¿½åŠ 
                    new_items = ""
                    for file_info in files:
                        rel_path = f'{file_info["category"]}/{file_info["filename"]}'
                        new_items += f'\n                        <li><a href="{rel_path}">{file_info["title"]}</a></li>'

                    return match.group(1) + str(new_count) + match.group(3) + new_items

                content = re.sub(pattern, replace_section, content)

                # å¤§ã‚«ãƒ†ã‚´ãƒªã®ãƒªã‚½ãƒ¼ã‚¹ã‚«ã‚¦ãƒ³ãƒˆã‚‚æ›´æ–°
                major_category_pattern = rf'(<div id="[^"]*{category}[^"]*" class="major-category">.*?<span class="resource-count">)(\d+)(</span>)'

                def update_major_count(match):
                    current_count = int(match.group(2))
                    new_count = current_count + len(files)
                    return match.group(1) + str(new_count) + match.group(3)

                content = re.sub(major_category_pattern, update_major_count, content, flags=re.DOTALL)

            if content != original_content:
                if not self.dry_run:
                    with open(self.index_html, 'w', encoding='utf-8') as f:
                        f.write(content)
                    print(f"âœ… index.html æ›´æ–°å®Œäº†")
                else:
                    print(f"[DRY RUN] index.html ã‚’æ›´æ–°")
                return True
            else:
                print("âš ï¸  index.html ã®æ›´æ–°ç®‡æ‰€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return False

        except Exception as e:
            print(f"âŒ index.html æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def integrate(self) -> bool:
        """çµ±åˆå‡¦ç†ã®ãƒ¡ã‚¤ãƒ³"""
        if not self.source_dir.exists():
            print(f"âŒ ã‚½ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.source_dir}")
            return False

        # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        html_files = list(self.source_dir.glob("*.html"))

        if not html_files:
            print(f"âš ï¸  {self.source_dir} ã«HTMLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False

        print(f"\nğŸ“‚ {len(html_files)}å€‹ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹\n")
        print("=" * 80)

        files_info = []

        # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æã—ã¦ç§»å‹•
        for html_file in html_files:
            print(f"\nğŸ“„ åˆ†æä¸­: {html_file.name}")

            metadata = self.analyze_html_file(html_file)
            if not metadata:
                continue

            print(f"   ğŸ“ ã‚¿ã‚¤ãƒˆãƒ«: {metadata['title']}")
            print(f"   ğŸ“Š ã‚µã‚¤ã‚º: {metadata['lines']:,} è¡Œ")

            # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®š
            category, section, icon = self.determine_category(metadata)
            print(f"   {icon} ã‚«ãƒ†ã‚´ãƒª: {category}/{section}")

            # ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•
            dest_file = self.move_file(html_file, category)

            files_info.append({
                "filename": html_file.name,
                "title": metadata["title"],
                "category": category,
                "section": section,
                "icon": icon,
                "dest": str(dest_file) if dest_file else None
            })

        print("\n" + "=" * 80)
        print(f"\nğŸ“‹ çµ±åˆã‚µãƒãƒªãƒ¼:")
        print(f"   âœ… å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files_info)}")

        category_counts = {}
        for info in files_info:
            cat = info["category"]
            category_counts[cat] = category_counts.get(cat, 0) + 1

        for category, count in sorted(category_counts.items()):
            print(f"   â€¢ {category}: {count}å€‹")

        # index.htmlæ›´æ–°
        print(f"\nğŸ“ index.html ã‚’æ›´æ–°ä¸­...")
        if self.update_index_html(files_info):
            print("âœ… çµ±åˆå®Œäº†ï¼")
        else:
            print("âš ï¸  index.html ã®æ›´æ–°ã«å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸ")

        # ç©ºã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤
        if not self.dry_run and not any(self.source_dir.iterdir()):
            self.source_dir.rmdir()
            print(f"\nğŸ—‘ï¸  ç©ºã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤: {self.source_dir}")

        return True


def main():
    parser = argparse.ArgumentParser(
        description="AWS SAPå­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹ã®è‡ªå‹•çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # new_html/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆ
  python3 integrate_new_html.py

  # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆå®Ÿéš›ã®å¤‰æ›´ãªã—ï¼‰
  python3 integrate_new_html.py --dry-run

  # ã‚«ã‚¹ã‚¿ãƒ ã‚½ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š
  python3 integrate_new_html.py --source custom_html/
        """
    )

    parser.add_argument(
        "--source", "-s",
        default="new_html",
        help="çµ±åˆå…ƒã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: new_htmlï¼‰"
    )

    parser.add_argument(
        "--dry-run", "-d",
        action="store_true",
        help="ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³: å®Ÿéš›ã®å¤‰æ›´ã‚’è¡Œã‚ãšã€å‡¦ç†å†…å®¹ã‚’è¡¨ç¤º"
    )

    args = parser.parse_args()

    print("ğŸ¤– AWS SAP å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹è‡ªå‹•çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 80)

    if args.dry_run:
        print("âš ï¸  ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰: å®Ÿéš›ã®å¤‰æ›´ã¯è¡Œã„ã¾ã›ã‚“\n")

    integrator = HTMLIntegrator(source_dir=args.source, dry_run=args.dry_run)
    success = integrator.integrate()

    if success and not args.dry_run:
        print("\n" + "=" * 80)
        print("ğŸ‰ çµ±åˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("  1. ãƒ­ãƒ¼ã‚«ãƒ«ã‚µãƒ¼ãƒãƒ¼ã§ç¢ºèª: python3 server.py")
        print("  2. Gitã«ã‚³ãƒŸãƒƒãƒˆ: git add . && git commit -m 'feat: æ–°è¦ãƒªã‚½ãƒ¼ã‚¹çµ±åˆ'")
        print("  3. ãƒªãƒ¢ãƒ¼ãƒˆã«ãƒ—ãƒƒã‚·ãƒ¥: git push origin gh-pages")

    return 0 if success else 1


if __name__ == "__main__":
    exit(main())
